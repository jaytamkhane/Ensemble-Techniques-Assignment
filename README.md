# Ensemble Learning Techniques – Jupyter Notebook Solutions

This repository contains comprehensive solutions to a set of theoretical and practical questions related to **Ensemble Learning Techniques**. The assignment focuses on methods such as **Bagging**, **Random Forest**, **Stacking**, and their applications in both classification and regression tasks.

## 📘 Assignment Overview

The notebook includes answers to:
- 📖 **Theoretical Questions** about the concepts, benefits, and challenges of ensemble methods.
- 🔬 **Practical Exercises** involving the implementation of various ensemble algorithms using Python and scikit-learn.

---

## 📂 Contents

### ✅ Theoretical Concepts
- What is ensemble learning?
- Types of ensemble techniques
- Bagging vs Boosting
- Feature randomness in Random Forest
- Out-of-Bag (OOB) score
- Feature importance in Random Forest
- Advantages and challenges of ensemble methods
- When to avoid ensemble methods

### 🛠️ Practical Implementations
- **Bagging Classifier** with Decision Tree, SVM, and Logistic Regression
- **Bagging Regressor** with Decision Tree and KNeighborsRegressor
- **Random Forest Classifier** on Breast Cancer dataset
- **Random Forest Regressor** comparison with single Decision Tree
- Hyperparameter tuning using **GridSearchCV**
- Model evaluation using:
  - Accuracy
  - MSE
  - Precision, Recall, F1-score
  - AUC and ROC curves
  - Confusion Matrix
- **Stacking Classifier** using combinations of models

---

## 🧰 Technologies Used

- Python
- Jupyter Notebook
- scikit-learn
- pandas, numpy
- matplotlib, seaborn

---

## 📊 Dataset Used

- Breast Cancer dataset (from `sklearn.datasets`)
- Sample synthetic datasets for regression/classification (where applicable)

---
